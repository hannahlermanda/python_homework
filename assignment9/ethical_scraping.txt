Task 5: Ethical Web Scraping
## Which sections of the website are restricted for crawling?
    Several sections of Wikipedia are restricted for webcrawling. 
    All bots are not allowed to scrape /w/, /api/, /trap/, /wiki/Special:, /wiki/Spezial:,/wiki/Spesial:, /wiki/Special%3A, /wiki/Spezial%3A, and /wiki/Spesial%3A.

    There are also several restrictions for crawling in the different languaged wikipedias. 
    Like the Italian wikipedia restricts scraping on /wiki/Wikipedia:Pagine_da_cancellare, /wiki/Wikipedia%3APagine_da_cancellare, /wiki/Wikipedia:Utenti_problematici, /wiki/Wikipedia%3AUtenti_problematici, /wiki/Wikipedia:Vandalismi_in_corso, /wiki/Wikipedia%3AVandalismi_in_corso, /wiki/Wikipedia:Amministratori, /wiki/Wikipedia%3AAmministratori, /wiki/Wikipedia:Proposte_di_cancellazione_semplificata, /wiki/Wikipedia%3AProposte_di_cancellazione_semplificata, /wiki/Categoria:Da_cancellare_subito, /wiki/Categoria%3ADa_cancellare_subito, /wiki/Wikipedia:Sospette_violazioni_di_copyright, /wiki/Wikipedia%3ASospette_violazioni_di_copyright, /wiki/Categoria:Da_controllare_per_copyright, /wiki/Categoria%3ADa_controllare_per_copyright, /wiki/Progetto:Rimozione_contributi_sospetti, /wiki/Progetto%3ARimozione_contributi_sospetti, /wiki/Categoria:Da_cancellare_subito_per_violazione_integrale_copyright, /wiki/Categoria%3ADa_cancellare_subito_per_violazione_integrale_copyright, /wiki/Progetto:Cococo, /wiki/Progetto%3ACococo, /wiki/Discussioni_progetto:Cococo, and /wiki/Discussioni_progetto%3ACococo.
    The English one has some similar and different restrictions such as, /wiki/Wikipedia:Articles_for_deletion/, /wiki/Wikipedia%3AArticles_for_deletion/, /wiki/Wikipedia:Votes_for_deletion/, /wiki/Wikipedia%3AVotes_for_deletion/, /wiki/Wikipedia:Pages_for_deletion/, /wiki/Wikipedia%3APages_for_deletion/, /wiki/Wikipedia:Miscellany_for_deletion/, /wiki/Wikipedia%3AMiscellany_for_deletion/, /wiki/Wikipedia:Miscellaneous_deletion/, /wiki/Wikipedia%3AMiscellaneous_deletion/, /wiki/Wikipedia:Copyright_problems, /wiki/Wikipedia%3ACopyright_problems, /wiki/Wikipedia:Protected_titles/, /wiki/Wikipedia%3AProtected_titles/, /wiki/Wikipedia:WikiProject_Spam/, /wiki/Wikipedia%3AWikiProject_Spam/, /wiki/MediaWiki:Spam-blacklist, /wiki/MediaWiki%3ASpam-blacklist, /wiki/MediaWiki_talk:Spam-blacklist, /wiki/MediaWiki_talk%3ASpam-blacklist, /wiki/Wikipedia:Requests_for_arbitration/, /wiki/Wikipedia%3ARequests_for_arbitration/, /wiki/Wikipedia:Requests_for_comment/, /wiki/Wikipedia%3ARequests_for_comment/, /wiki/Wikipedia:Requests_for_adminship/, /wiki/Wikipedia%3ARequests_for_adminship/, /wiki/Wikipedia_talk:Articles_for_deletion/, /wiki/Wikipedia_talk%3AArticles_for_deletion/, /wiki/Wikipedia_talk:Votes_for_deletion/, /wiki/Wikipedia_talk%3AVotes_for_deletion/, /wiki/Wikipedia_talk:Pages_for_deletion/, /wiki/Wikipedia_talk%3APages_for_deletion/, /wiki/Wikipedia_talk:Miscellany_for_deletion/, /wiki/Wikipedia_talk%3AMiscellany_for_deletion/, /wiki/Wikipedia_talk:Miscellaneous_deletion/, /wiki/Wikipedia_talk%3AMiscellaneous_deletion/, /wiki/Wikipedia:Changing_username, /wiki/Wikipedia%3AChanging_username, /wiki/Wikipedia:Changing_username/, /wiki/Wikipedia%3AChanging_username/, /wiki/Wikipedia_talk:Changing_username, /wiki/Wikipedia_talk%3AChanging_username, /wiki/Wikipedia_talk:Changing_username/, and /wiki/Wikipedia_talk%3AChanging_username/.
    The Spanish (Spain) one has way less restrictions including only /wiki/Wikipedia:Consultas_de_borrado/ and /wiki/Wikipedia%3AConsultas_de_borrado/.
## Are there specific rules for certain user agents?
    Some bots are blocked from everything, such as MJ12bot and Download Ninja, fast, NPBot, and grub-client, among several others.
    wget is restricted in its recursive mode and may be allowed if the user uses the --wait option.
    According to the robots.txt, "friendly, low-speed bots are welcome viewing article pages, but not dynamically-generate pages".

##Reflect on why websites use robots.txt and write 2-3 sentences explaining its purpose and how it promotes ethical scraping.
    Websites use robot.txt to protect the data they have on their websites and to ensure that their servers don't get overloaded.
    Like in Wikipedia's robots.txt, some pages are restricted from being scraped to 'protect' people's identities online (like preventing certain pages from popping up as the first results hen you Google somebody's name).
    Too many robots scraping websites at the same time would also probably slow a website's performance down signficantly. 